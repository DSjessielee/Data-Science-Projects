{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02CfYifUSHzJ"
   },
   "source": [
    "# Question 1: Bias-Variance (5 pts)\n",
    "\n",
    "a) (**2 pts**) Assume there is a data generator $Y=f(X)+\\epsilon$, which is generating Data(X, Y), where $\\epsilon$ is the added random gaussian noise. We are trying to fit a curve to the samples generated from the data generator, using an estimator. The estimator can be represented as $g(X|\\theta)$, where $\\theta$ represents the parameters. For any test point $x_0$, what does the following mathematical representation mean? Is this the bias or variance of the estimator? $$E[g(x_0)]-f(x_0)$$\n",
    "\n",
    "b) (**3 pts**) Use your own words to describe why there is a tradeoff between bias and variance. \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)This function represents the difference between estimated value by the model and the true value at certain point $x_0$ from the generator fuction. This is the bias of the estimator at $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Bias describes how good the average model is, which is quantitatively how far off your modeled mean vs true mean of samples, where variance describes how sensitive the model is to variations in data. When a model is simple (in the Lasso or Ridge case, regularization is high), the bias is high but the variance is low, where each variable is given low weights; whereas for high complexity models (in the Lasso or Ridge case, regularization is low), for example overfit models, variance is high but the bias is low where training samples were all given high weight. Expected loss (E[L]) = $bias^2$ + variance + noise, so at low model complexity, E(L) is dominated by bias where at high complexity, E[L] is dominated by variance. What we need to do is to find the Goldilocks zone between the trade offs when we build our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JX0hdXUSJJj"
   },
   "source": [
    "# Question 2: Bias-Variance exploration (20 pts)\n",
    "\n",
    "We want to build a model that can predict y for unknown inputs x.\n",
    "\n",
    "(a) (**10 pts**) Fit polynomial models of degrees 2, 4, 7 to the training data. Print out the mean squared error (on both train and test sets) for all the models. Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test). All the plots must be in the same figure and be clearly labeled. **Tips**: you can use `np.vander(np.squeeze(x_train), deg+1)` to generate the `deg`-degree polynomial vector of `x_train`. For example, `np.vander(np.squeeze(x_train), 3)` gives you the second-degree polynomial of `x_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7CEX0XdwSo0A"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2dbb448cb730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdata_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Xtrain\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ytrain\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.npy'"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "%matplotlib inline\n",
    "\n",
    "data_load = np.load('data.npy', allow_pickle=True)\n",
    "x_train = data_load.item().get(\"Xtrain\")\n",
    "y_train = data_load.item().get(\"Ytrain\")\n",
    "x_test = data_load.item().get(\"Xtest\")\n",
    "y_test =data_load.item().get(\"Ytest\")\n",
    "x_all = np.linspace(-10,10,101).reshape(-1,1)\n",
    "\n",
    "lrp = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "x_test = x_test.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "x_all = np.linspace(-10,10,101).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtseLbFDTLws"
   },
   "source": [
    "### Answer a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit polynomial models of degrees 2, 4, 8 to the training data\n",
    "\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "poly4 = PolynomialFeatures(degree=4)\n",
    "poly7 = PolynomialFeatures(degree=7)\n",
    "\n",
    "X_2_train = poly2.fit_transform(x_train)\n",
    "X_4_train = poly4.fit_transform(x_train)\n",
    "X_7_train = poly7.fit_transform(x_train)\n",
    "\n",
    "X_2_test = poly2.fit_transform(x_test)\n",
    "X_4_test = poly4.fit_transform(x_test)\n",
    "X_7_test = poly7.fit_transform(x_test)\n",
    "\n",
    "lrp2 = LinearRegression()\n",
    "lrp2.fit(X_2_train, y_train)\n",
    "y_2_train = lrp2.predict(X_2_train)\n",
    "y_2_test = lrp2.predict(X_2_test)\n",
    "\n",
    "lrp4 = LinearRegression()\n",
    "lrp4.fit(X_4_train, y_train)\n",
    "y_4_train = lrp4.predict(X_4_train)\n",
    "y_4_test = lrp4.predict(X_4_test)\n",
    "\n",
    "\n",
    "lrp7 = LinearRegression()\n",
    "lrp7.fit(X_7_train, y_train)\n",
    "y_7_train = lrp7.predict(X_7_train)\n",
    "y_7_test = lrp7.predict(X_7_test)\n",
    "\n",
    "\n",
    "df_train=pd.DataFrame(x_train)\n",
    "\n",
    "df_train['y_train']   = y_train\n",
    "df_train['y_2_train'] = y_2_train\n",
    "df_train['y_4_train'] = y_4_train\n",
    "df_train['y_7_train'] = y_7_train\n",
    "df_train.columns=['x_train','y_train','y_2_train', 'y_4_train', 'y_7_train']\n",
    "df_train.sort_values(by='x_train',inplace=True)\n",
    "\n",
    "df_test=pd.DataFrame(x_test)\n",
    "\n",
    "df_test['y_test'] = y_test\n",
    "df_test['y_2_test'] = y_2_test\n",
    "df_test['y_4_test'] = y_4_test\n",
    "df_test['y_7_test'] = y_7_test\n",
    "df_test.columns=['x_test','y_test','y_2_test', 'y_4_test', 'y_7_test']\n",
    "df_test.sort_values(by='x_test',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE for degree 2 polynomial on training data is: ', mse(y_train,y_2_train))\n",
    "print('MSE for degree 4 polynomial on training data is: ', mse(y_train,y_4_train))\n",
    "print('MSE for degree 7 polynomial on training data is: ', mse(y_train,y_7_train))\n",
    "print('MSE for degree 2 polynomial on test data is: ', mse(y_test,y_2_test))\n",
    "print('MSE for degree 4 polynomial on test data is: ', mse(y_test,y_4_test))\n",
    "print('MSE for degree 7 polynomial on test data is: ', mse(y_test,y_7_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all model predictions vs. test data\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(df_train['x_train'], df_train['y_train'],linestyle='None',marker= 'o', color='b',label='train_obs')\n",
    "ax.plot(df_train['x_train'], df_train['y_2_train'],linestyle='-',color='m',label='2nd deg poly pred on train')\n",
    "ax.plot(df_train['x_train'], df_train['y_4_train'],linestyle='-',color='c',label='4th deg poly pred on train')\n",
    "ax.plot(df_train['x_train'], df_train['y_7_train'],linestyle='-',color='y',label='7th deg poly pred on tain')\n",
    "\n",
    "ax.plot(df_test['x_test'], df_test['y_test'],linestyle='None',marker= 's', color='r',label='train_obs')\n",
    "ax.plot(df_test['x_test'], df_test['y_2_test'],linestyle='-.',color='m',label='2nd deg poly pred on test')\n",
    "ax.plot(df_test['x_test'], df_test['y_4_test'],linestyle='-.',color='c',label='4th deg poly pred on test')\n",
    "ax.plot(df_test['x_test'], df_test['y_7_test'],linestyle='-.',color='y',label='7th deg poly pred on test')\n",
    "\n",
    "\n",
    "ax.set_xscale('linear')\n",
    "ax.set_xlim(ax.get_xlim()[::1])  # normal direction \n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Model Comparison')\n",
    "ax.legend()\n",
    "plt.axis('tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (**5 pts**) Which model gives the best performance? Explain in terms of the bias-variance tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, for training data, 7th degree polynomial model gives the best result based on MSE, because when increasing model complexity and test the error on the data that was used to fit the model, you always get tighter 'fit' give you lower MSE with no penalty for overfitting. However, when you test your model with test data. you will see a trade off between bias and variance, when you look at the loss function loss (E[L]) = $bias^2$ + variance + noise, where when you cross the optimal complexity and complexity is too high for the given sample size, increased variance will increase with complexity resulting in higher predition error. For our case, 2nd degree and 7th degree polynomial fit gave higher MSE than 4th degree polynomial, which indicate that 4th degree polynomial fit is close to the optimal degree of model complexity for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (**5 pts**) Analyse how the training data size affects the bias and the variance of the models. For this, run the analysis in (a) using 20, 40, 60, 80 and all 100 data points. Make a **single** plot of the $log(MSE)$ for both the training and the test data vs the size of the training set for each of the polynomials. State the trends you see as you change the size of the training data on each of the models and explain why you see them.\n",
    "**You can use the following codes to load the dataset and complete the analysis**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(x_train,y_train, x_test, y_test):\n",
    "\n",
    "\n",
    "        poly2 = PolynomialFeatures(degree=2)\n",
    "        poly4 = PolynomialFeatures(degree=4)\n",
    "        poly7 = PolynomialFeatures(degree=7)\n",
    "\n",
    "        X_2_train = poly2.fit_transform(x_train)\n",
    "        X_4_train = poly4.fit_transform(x_train)\n",
    "        X_7_train = poly7.fit_transform(x_train)\n",
    "\n",
    "        X_2_test = poly2.fit_transform(x_test)\n",
    "        X_4_test = poly4.fit_transform(x_test)\n",
    "        X_7_test = poly7.fit_transform(x_test)\n",
    "\n",
    "        lrp2 = LinearRegression()\n",
    "        lrp2.fit(X_2_train, y_train)\n",
    "        y_2_train = lrp2.predict(X_2_train)\n",
    "        y_2_test = lrp2.predict(X_2_test)\n",
    "\n",
    "        lrp4 = LinearRegression()\n",
    "        lrp4.fit(X_4_train, y_train)\n",
    "        y_4_train = lrp4.predict(X_4_train)\n",
    "        y_4_test = lrp4.predict(X_4_test)\n",
    "\n",
    "\n",
    "        lrp7 = LinearRegression()\n",
    "        lrp7.fit(X_7_train, y_train)\n",
    "        y_7_train = lrp7.predict(X_7_train)\n",
    "        y_7_test = lrp7.predict(X_7_test)\n",
    "        \n",
    "        df_train=pd.DataFrame(x_train)\n",
    "\n",
    "        df_train['y_train']   = y_train\n",
    "        df_train['y_2_train'] = y_2_train\n",
    "        df_train['y_4_train'] = y_4_train\n",
    "        df_train['y_7_train'] = y_7_train\n",
    "        df_train.columns=['x_train','y_train','y_2_train', 'y_4_train', 'y_7_train']\n",
    "        df_train.sort_values(by='x_train',inplace=True)\n",
    "\n",
    "        df_test=pd.DataFrame(x_test)\n",
    "\n",
    "        df_test['y_test'] = y_test\n",
    "        df_test['y_2_test'] = y_2_test\n",
    "        df_test['y_4_test'] = y_4_test\n",
    "        df_test['y_7_test'] = y_7_test\n",
    "        df_test.columns=['x_test','y_test','y_2_test', 'y_4_test', 'y_7_test']\n",
    "        df_test.sort_values(by='x_test',inplace=True)\n",
    "\n",
    "\n",
    "        ax = plt.gca()\n",
    "\n",
    "        ax.plot(df_train['x_train'], df_train['y_train'],linestyle='None',marker= 'o', color='b',label='train_obs')\n",
    "        ax.plot(df_train['x_train'], df_train['y_2_train'], linestyle='-',color='m',label='2nd deg poly pred on train')\n",
    "        ax.plot(df_train['x_train'], df_train['y_4_train'], linestyle='-',color='c',label='4th deg poly pred on train')\n",
    "        ax.plot(df_train['x_train'], df_train['y_7_train'], linestyle='-',color='y',label='7th deg poly pred on tain')\n",
    "\n",
    "        ax.plot(df_test['x_test'], df_test['y_test'],linestyle='None',marker= 's', color='r',label='train_obs')\n",
    "        ax.plot(df_test['x_test'], df_test['y_2_test'], linestyle='-.',color='m',label='2nd deg poly pred on test')\n",
    "        ax.plot(df_test['x_test'], df_test['y_4_test'], linestyle='-.',color='c',label='4th deg poly pred on test')\n",
    "        ax.plot(df_test['x_test'], df_test['y_7_test'], linestyle='-.',color='y',label='7th deg poly pred on test')\n",
    "\n",
    "\n",
    "        ax.set_xscale('linear')\n",
    "        ax.set_xlim(ax.get_xlim()[::1])  # normal direction \n",
    "        plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Model Comparison')\n",
    "        ax.legend()\n",
    "        plt.axis('tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return mse(y_train,y_2_train),mse(y_test,y_2_test),mse(y_train,y_4_train),mse(y_test,y_4_test),mse(y_train,y_7_train),mse(y_test,y_7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"20% data\")\n",
    "train2_rmses_20, test2_rmses_20, train4_rmses_20, test4_rmses_20,train7_rmses_20, test7_rmses_20 = plot_curves(x_train[40:60], y_train[40:60], x_test, y_test)\n",
    "print(\"40% data\")\n",
    "train2_rmses_40, test2_rmses_40,train4_rmses_40, test4_rmses_40,train7_rmses_40, test7_rmses_40 = plot_curves(x_train[30:70], y_train[30:70], x_test, y_test)\n",
    "print(\"60% data\")\n",
    "train2_rmses_60, test2_rmses_60,train4_rmses_60, test4_rmses_60,train7_rmses_60, test7_rmses_60 = plot_curves(x_train[20:80], y_train[20:80], x_test, y_test)\n",
    "print(\"80% data\")\n",
    "train2_rmses_80, test2_rmses_80,train4_rmses_80, test4_rmses_80,train7_rmses_80, test7_rmses_80 = plot_curves(x_train[10:90], y_train[10:90], x_test, y_test)\n",
    "print(\"100% data\")\n",
    "train2_rmses_100, test2_rmses_100,train4_rmses_100, test4_rmses_100,train7_rmses_100, test7_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrse = pd.DataFrame({\n",
    "                     'type':['train','train','train','test','test','test'],\n",
    "                     '20% data':[np.log(train2_rmses_20), np.log(train4_rmses_20), np.log(train7_rmses_20),\n",
    "                                 np.log(test2_rmses_20), np.log(test4_rmses_20), np.log(test7_rmses_20)],\n",
    "                     '40% data':[np.log(train2_rmses_40), np.log(train4_rmses_40), np.log(train7_rmses_40),\n",
    "                                 np.log(test2_rmses_40), np.log(test4_rmses_40), np.log(test7_rmses_40)],\n",
    "                     '60% data':[np.log(train2_rmses_60), np.log(train4_rmses_60), np.log(train7_rmses_60),\n",
    "                                 np.log(test2_rmses_60), np.log(test4_rmses_60), np.log(test7_rmses_60)],\n",
    "                     '80% data':[np.log(train2_rmses_80), np.log(train4_rmses_80), np.log(train7_rmses_80),\n",
    "                                 np.log(test2_rmses_80), np.log(test4_rmses_80), np.log(test7_rmses_80)],\n",
    "                     '100% data':[np.log(train2_rmses_100), np.log(train4_rmses_100), np.log(train7_rmses_100),\n",
    "                                  np.log(test2_rmses_100), np.log(test4_rmses_100), np.log(test7_rmses_100)],\n",
    "                      },\n",
    "                      index= ['degree=2 train', 'degree=4 train', 'degree=7 train',\n",
    "                              'degree=2 test', 'degree=4 test', 'degree=7 test'])\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "mrse.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, looking at this plot, we know that the performances of the models are not one-to-one across training and testing data. What this plot reveals is what we know about the bias-variance tradeoff.\n",
    "\n",
    "For the least complex model, the second-degree polynomial, we see that there is a larger amount of bias, than the other models, in keeping with what we know about the bias-variance tradeoff. We see that for the most complex model (the seventh-degree polynomial), that on the train data, as the percentage of data increases, so does the MSE and on the test data, as the percentage of data increases, the MSE decreases. What this is saying is that there is lower bias, but higher variance compared to the other (less complex) models. Finally, the model that strikes a more optimal balance between the two in the tradeoff is the mid-most complex model, the fourth-degree polynomial. \n",
    "\n",
    "A key takeaway is that as the data size increases, the variance decreases across all models. This is because the models can better learn patterns that will perform well on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKbRSU9tSoRD"
   },
   "source": [
    "# Question 3: Gradient descent (5 pts)\n",
    "\n",
    "a) (**2 pts**) Compare gradient descent and stochastic gradient descent in terms of their key advantages and disdvantages. Limit your answer to one paragaraph.\n",
    "\n",
    "b) (**3 pts**) Read this [blog](https://www.benfrederickson.com/numerical-optimization/) on second order optimization and answer the following question: **How does Nelder-Mead method work? What can be the major problems of Nelder-Mead method?**\n",
    "\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both gradient descent (GD) and stochastic gradient descent (SGD), a set of model parameters are updated iteratively to minimize an error function. While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. Thus, if the number of training samples are large, using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. Further, gradient descent can take too long if the learning late is too low. If it's too high, gradient descent will vary and possibly not converge. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample. However, because the SGD updates for every instance in an \"on-line\" version, if there are very frequent updates, SGD can become computationally expensive. And, SGD is more expensive when applied to very simple models on very small datasets. SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Nelder-Mead' method is the algorithm to find the minimum of a function by taking repeated steps down towards minima, and dynamically adjusts the step size based off the loss of the new point. If the new point is better than any previously seen value, it expands the step size to accelerate towards the bottom. Likewise if the new point is worse it contracts the step size to converge around the minima. This method can be easily extended into higher dimensional examples, all that's required is taking one more point than there are dimensions, reflecting the worst point around the rest of the points to take a step down.\n",
    "\n",
    "The biggest downside of 'Nelder-Mead' as a direct search method is to deal with high dimensional functions as it's exponentially more difficult to figure out the 'direction' when dimentionality grows. As the result, 'Nelder-Mead' performs well on 1 and 2 dimensional data, however, it won't work well on even simple problems with more than a dozen or so parameters, not to mention models with millions if not billions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ezhiR4cSsUs"
   },
   "source": [
    "# Question 4: Stochastic gradient descent (10 pts)\n",
    "\n",
    "Use stochastic gradient descent to derive the coefficent updates for the 4 coefficients $w_0, w_1, w_2, w_3$ in this modelï¼š\n",
    "$$ y = w_0 + w_1x_1 + w_2 x_1x_2 + w_3e^{-x_1} $$ \n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression\n",
    "\n",
    "$\\hat{Y} = \\hat{\\beta}_{0} + \\sum \\limits _{j=1} ^{n} X_{j}\\hat{\\beta}_{j} $    ..........(1)\n",
    "\n",
    "Here $n=3$ where\n",
    "\n",
    "equation (1) becomes\n",
    "\n",
    "$\\hat{Y} = \\sum \\limits _{j=0} ^{3} X_{j}\\hat{\\beta}_{j} $    ..........(2)\n",
    "\n",
    "where x0=1\n",
    "\n",
    "\n",
    "\n",
    "(2) can be written as matrix form\n",
    "\n",
    "$\\hat{Y}=W*X^T$ Where\n",
    "\n",
    "\n",
    "$$W=\\begin{bmatrix} {\\beta}_{0} & {\\beta}_{1} & {\\beta}_{2} & {\\beta}_{3} \\end{bmatrix}$$ and $$X=\\begin{bmatrix} 1 & x1 & x1x2 & e^{-x_1} \\end{bmatrix}$$      \n",
    "\n",
    "\n",
    "Using simple square error loss function J to minimize J by updating ${\\beta}_{j}$\n",
    "\n",
    "$J(x,y,{\\beta})= \\sum \\limits _{j=0} ^{3} (x_{j}{\\beta}_{j}-y_{j})^2 $ ...........................(3)\n",
    "\n",
    "\n",
    "\n",
    "Gradient is\n",
    "\n",
    "$\\frac {dJ}{dW}= 2\\sum \\limits _{j=0} ^{3} (x_{j}{\\beta}_{j}-y_{j})x_{j} $  ..........................(4)\n",
    "\n",
    "\n",
    "\n",
    "In matrix form, gradient $\\eta$\n",
    "\n",
    "$\\eta=(\\hat{y}_{i}-y_{i})X$\n",
    "\n",
    "\n",
    "For SGD, we want to get the gradient of W from partial partial differentiation of ${\\beta}_{j}$ updated from iterating each additional sample by regulating gradient with a learning rate $\\alpha$\n",
    "\n",
    "so each of the 500 samples (denoted as i here)\n",
    "\n",
    "${\\beta}_{i+1}={\\beta}_{i}-{\\alpha}\\frac {dJ}{\\beta}$\n",
    "\n",
    "\n",
    "${\\beta}_{i+1}={\\beta}_{i}-{\\alpha}\\sum \\limits _{j=0} ^{3} (x_{ij}{\\beta}_{j}-y_{ij})x_{ij}$\n",
    "\n",
    "\n",
    "In our specific case, for each sample i:\n",
    "\n",
    "${\\beta}_{0(i+1)} ={\\beta}_{0i)}+{\\alpha}{\\beta}_{0}x0_i$\n",
    "\n",
    "Since X0=1, so for each sample i\n",
    "\n",
    "${\\beta}_{0(i+1)} ={\\beta}_{0i}-{\\alpha}(\\hat{y}_{i}-y_{i})$\n",
    "\n",
    "And \n",
    "\n",
    "$\\\\{\\beta}_{1(i+1)} ={\\beta}_{1i}-{\\alpha}(\\hat{y}_{i}-y_{i})*x1_{i}$\n",
    "$\\\\{\\beta}_{2(i+1)} ={\\beta}_{2i}-{\\alpha}(\\hat{y}_{i}-y_{i})*x2_{i}$\n",
    "$\\\\{\\beta}_{3(i+1)} ={\\beta}_{3i}-{\\alpha}(\\hat{y}_{i}-y_{i})*x3_{i}$\n",
    "\n",
    "in vector form\n",
    "\n",
    "$W_{i+1}=W_i-{\\alpha}\\eta$\n",
    "\n",
    "Where $\\eta=(\\hat{y}_{i}-y_{i})X$\n",
    "\n",
    "So $W_{i+1}=W_i-{\\alpha}(\\hat{y}_{i}-y_{i})X$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For Ridge Model (3) becomes\n",
    "\n",
    "$J(x,y,{\\beta})= \\sum \\limits _{j=0} ^{3} (x_{j}{\\beta}_{j}-y_{j})^2 + {\\lambda}\\sum \\limits _{j=1} ^{3}{\\beta}_j^2$ ... (5)\n",
    "\n",
    "where ${\\lambda}$ is the regulation factor\n",
    "\n",
    "(4) becomes\n",
    "\n",
    "\n",
    "$\\frac {dJ}{dW}= \\sum \\limits _{j=0} ^{3} (x_{j}{\\beta}_{j}-y_{j})x_{j} + {\\lambda}\\sum \\limits _{j=1} ^{3}{\\beta}_j$  ..........................(6)\n",
    "\n",
    "\n",
    "where constant 2 can be omitted\n",
    "\n",
    "Since X0=1, so for each sample i, we dont need to regulate ${\\beta}_{0}$\n",
    "\n",
    "${\\beta}_{0(i+1)} ={\\beta}_{0i}-{\\alpha}(\\hat{y}_{i}-y_{i})$\n",
    "\n",
    "And \n",
    "\n",
    "$\\\\{\\beta}_{1(i+1)} ={\\beta}_{1(i)}-{\\alpha}[(\\hat{y}_{i}-y_{i})*x1_{i}+{\\lambda}{\\beta}_{1i}]$\n",
    "$\\\\{\\beta}_{2(i+1)} ={\\beta}_{2(i)}-{\\alpha}[(\\hat{y}_{i}-y_{i})*x2_{i}+{\\lambda}{\\beta}_{2i}]$\n",
    "$\\\\{\\beta}_{3(i+1)} ={\\beta}_{3(i)}-{\\alpha}[(\\hat{y}_{i}-y_{i})*x3_{i}+{\\lambda}{\\beta}_{3i}]$\n",
    "\n",
    "\n",
    "In vector form, update W\n",
    "\n",
    "$W_{i+1}=W_{i}-\\alpha\\eta$\n",
    "\n",
    "Where gradient $\\eta$\n",
    "\n",
    "$\\eta=(\\hat{y}_{i}-y_{i})X_{i}+{\\lambda}W^{*}_{i}$\n",
    "\n",
    "where here because $W_{0}={\\beta}_{0}$ is not regulated so  $W_{0}$ is set to be 0 noted as $W^{*}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ8eqNmwTWsK"
   },
   "source": [
    "# Question 5: Stochastic gradient descent coding (20 pts)\n",
    "\n",
    "Code an SGD solution in Python for this non-linear model$$ y = w_0 + w_1x_1 + w_2x_1x_2 + w_3e^{-x_1} $$  The template of the solution class is given. The init function of the class takes as input the learning rate, regularization constant and number of epochs. The fit method must take as input X, y. The predict method takes an X value (optionally, an array of values). \n",
    "\n",
    "a) (**15 pts**) Use your new gradient descent regression to predict the data given in 'SGD_samples.csv', for 15 epochs, using learning rates: [0, .0001, .001, .01, 0.1, 1, 10, 100] and regularization (ridge regression) constants: [0,10,100]. For the best 2 combinations of learning_rate and regularization for SGD, plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) . \n",
    "\n",
    "b) (**5 pts**) Report the MSE of the two best combinations of learning rate and regularization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IN4rIn-CSr0i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l676O-DJUKUM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate, regularization, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.regularization = regularization\n",
    "        # initialize whichever variables you would need here\n",
    "        self.coef = np.zeros(4)\n",
    "        \n",
    "    def sgd(self, gradient):\n",
    "        self.coef = self.coef - self.learning_rate * gradient\n",
    "                   \n",
    "    \n",
    "    def fit(self, X, y, update_rule='sgd', plot=True):\n",
    "        mse = []\n",
    "        coefs = []\n",
    "        X = self.get_features(X)\n",
    "        for epoch in range(self.n_epoch):\n",
    "        \n",
    "            for i in range(X.shape[0]):\n",
    "                \n",
    "                # Compute error\n",
    "                   #please put your codes here\n",
    "                yhat=self.linearPredict(X[i,:])\n",
    "                error = yhat - y[i]\n",
    "                # Compute gradients\n",
    "                    #please put your codes here\n",
    "                gradient= np.dot(error,X[i,:])+ np.append(np.zeros((1,1)),np.dot(self.regularization, self.coef[1:]))\n",
    "            \n",
    "                # Update weights\n",
    "                self.sgd(gradient)\n",
    "                \n",
    " #Go through each sample to update weight matrix using gradient and learning rate\n",
    "                     \n",
    "            coefs.append(self.coef)\n",
    "            residuals = y - self.linearPredict(X)         \n",
    "            mse.append(np.mean(residuals**2))\n",
    "\n",
    "        self.lowest_mse = mse[-1]\n",
    "        if plot == True:\n",
    "            plt.figure()\n",
    "            plt.plot(range(self.n_epoch),mse)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.figure()\n",
    "            coefs = np.array(coefs)\n",
    "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('parameter value')\n",
    "\n",
    "    def get_features(self, X):\n",
    "        '''\n",
    "        this output of this function can be used to compute the gradient in `fit`\n",
    "        '''\n",
    "        x = np.zeros((X.shape[0], 4))\n",
    "        x[:,0] = 1\n",
    "        x[:,1] = X[:,0]\n",
    "        x[:,2] = X[:,0]*X[:,1]\n",
    "        x[:,3] = np.exp(-X[:,0])\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def linearPredict(self, X):\n",
    "        #compute the dot product of self.coef and X\n",
    "        return np.dot(self.coef,X.T) #this line is just a placeholder, please delete this line in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CvOp8kdxUNCP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-091fbf17dc96>:17: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.coef = self.coef - self.learning_rate * gradient\n",
      "<ipython-input-3-091fbf17dc96>:43: RuntimeWarning: overflow encountered in square\n",
      "  mse.append(np.mean(residuals**2))\n",
      "<ipython-input-3-091fbf17dc96>:17: RuntimeWarning: overflow encountered in multiply\n",
      "  self.coef = self.coef - self.learning_rate * gradient\n"
     ]
    }
   ],
   "source": [
    "# Define X and y matrix\n",
    "df=pd.read_csv('SGD_samples-1.csv')\n",
    "y=np.array(df['y'])\n",
    "X=np.array(df[['x1','x2']])\n",
    "\n",
    "n_epochs = 15\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "regularization = [0, 10, 100]\n",
    "\n",
    "mse_comp=pd.DataFrame(np.zeros((len(learning_rate)*len(regularization),3)))\n",
    "row=0\n",
    "\n",
    "for l_rate in learning_rate:\n",
    "    for regular in regularization:\n",
    "        Reg=Regression(learning_rate=l_rate, regularization=regular, n_epoch=1000)\n",
    "        Reg.fit(X, y, plot=False)\n",
    "        mse_comp.iloc[row,0]=l_rate\n",
    "        mse_comp.iloc[row,1]=regular\n",
    "        mse_comp.iloc[row,2]=Reg.lowest_mse\n",
    "        row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regulation</th>\n",
       "      <th>lowest_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.033694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.033706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.033787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.033796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.033812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.033824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.034553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  regulation  lowest_mse\n",
       "3          0.0010         0.0    0.027360\n",
       "6          0.0100         0.0    0.027360\n",
       "0          0.0001         0.0    0.027415\n",
       "9          0.1000         0.0    0.030858\n",
       "1          0.0001        10.0    0.033694\n",
       "4          0.0010        10.0    0.033706\n",
       "2          0.0001       100.0    0.033787\n",
       "5          0.0010       100.0    0.033796\n",
       "8          0.0100       100.0    0.033812\n",
       "7          0.0100        10.0    0.033824\n",
       "10         0.1000        10.0    0.034553"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_comp.columns=['learning_rate','regulation','lowest_mse']\n",
    "mse_comp.dropna().sort_values(by='lowest_mse',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg=Regression(learning_rate=0.0010, regularization=0, n_epoch=n_epochs)\n",
    "Reg.fit(X, y, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg=Regression(learning_rate=0.01, regularization=0, n_epoch=n_epochs)\n",
    "Reg.fit(X, y, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First Best Combination: Learning Rate of 0.0010 and Regularization of 0')\n",
    "print('Second Best Combination: Learning Rate of 0.010 and Regularization of 0')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIS382N-HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
